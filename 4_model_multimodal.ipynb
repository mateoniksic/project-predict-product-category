{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85f27a99-9886-4abb-8fc1-55706a347f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "#Loading data, class definitions\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor, Normalize, Resize, Compose\n",
    "import torchvision\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import random\n",
    "import requests\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "class MultimodalClassifier(nn.Module):\n",
    "    def __init__(self, text_vocab_size, text_embedding_dim, text_seq_length, num_classes):\n",
    "        super(MultimodalClassifier, self).__init__()\n",
    "\n",
    "        # Image model (ResNet50)\n",
    "        self.image_model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n",
    "        for param in self.image_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.image_model.fc = nn.Linear(2048, 512)\n",
    "\n",
    "        # Text model\n",
    "        self.text_embedding = nn.Embedding(text_vocab_size, text_embedding_dim)\n",
    "        self.text_conv = nn.Conv1d(in_channels=text_embedding_dim, out_channels=256, kernel_size=5, padding=2)\n",
    "        self.text_lstm = nn.LSTM(input_size=256, hidden_size=256, batch_first=True, bidirectional=True)\n",
    "        self.text_fc = nn.Linear(512, 512)\n",
    "\n",
    "        # Combined classifier\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        # Image branch\n",
    "        image_features = self.image_model(image)\n",
    "    \n",
    "        # Text branch\n",
    "        text_embedded = self.text_embedding(text)\n",
    "        text_embedded = text_embedded.permute(0, 2, 1)\n",
    "    \n",
    "        # Apply the convolution\n",
    "        text_conv_out = self.text_conv(text_embedded)\n",
    "        \n",
    "        # Apply LSTM\n",
    "        text_features, _ = self.text_lstm(text_conv_out)\n",
    "        text_features = self.text_fc(text_features[:, -1, :])\n",
    "    \n",
    "        # Combine features\n",
    "        combined_features = torch.cat((image_features, text_features), dim=1)\n",
    "        return self.fc(combined_features)\n",
    "\n",
    "\n",
    "# Data preparation for images\n",
    "image_dataset_path = \"./datasets/dataset_clean/dataset_clean.csv\"\n",
    "image_dataset_images_path = Path(\"./datasets/dataset_clean/images/\")\n",
    "\n",
    "image_dataset = pd.read_csv(image_dataset_path)\n",
    "\n",
    "indices = list(range(len(image_dataset)))\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "image_dataset['image_path'] = image_dataset['image_path'].apply(lambda x: image_dataset_images_path / Path(x).name)\n",
    "image_categories = sorted(image_dataset['category'].unique())\n",
    "\n",
    "image_train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "image_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "image_train_data = image_dataset.iloc[train_indices]\n",
    "image_test_data = image_dataset.iloc[test_indices]\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = row['image_path']\n",
    "        label = row['category_encoded']\n",
    "\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Image not found: {image_path}\")\n",
    "            raise\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "image_train_dataset = ImageDataset(image_train_data, transform=image_train_transform)\n",
    "image_test_dataset = ImageDataset(image_test_data, transform=image_test_transform)\n",
    "\n",
    "image_train_loader = DataLoader(image_train_dataset, batch_size=64, shuffle=False)\n",
    "image_test_loader = DataLoader(image_test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Data preparation for text\n",
    "text_dataset_path = './datasets/dataset_clean/dataset_clean.csv'\n",
    "text_dataset = pd.read_csv(text_dataset_path)\n",
    "\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "text_tokenizer = {}\n",
    "word_index = {}\n",
    "\n",
    "def build_tokenizer_and_vocab(descriptions, vocab_size):\n",
    "    from collections import Counter\n",
    "    global text_tokenizer, word_index\n",
    "    tokenizer = Counter()\n",
    "    for desc in descriptions:\n",
    "        tokenizer.update(desc.lower().split())\n",
    "    most_common = tokenizer.most_common(vocab_size - 1)\n",
    "    text_tokenizer = {word: i + 1 for i, (word, _) in enumerate(most_common)}\n",
    "    word_index = {word: i + 1 for i, (word, _) in enumerate(most_common)}\n",
    "\n",
    "def tokenize_text(description, max_length):\n",
    "    global text_tokenizer\n",
    "    tokens = [text_tokenizer.get(word, 0) for word in description.lower().split()]\n",
    "    if len(tokens) < max_length:\n",
    "        tokens += [0] * (max_length - len(tokens))\n",
    "    return tokens[:max_length]\n",
    "\n",
    "build_tokenizer_and_vocab(text_dataset['description'], VOCAB_SIZE)\n",
    "text_dataset['tokenized'] = text_dataset['description'].apply(lambda x: tokenize_text(x, MAX_LENGTH))\n",
    "\n",
    "text_X = np.array(text_dataset['tokenized'].tolist())\n",
    "text_y = np.array(text_dataset['category_encoded'])\n",
    "\n",
    "text_X_train = text_X[train_indices]\n",
    "text_X_test = text_X[test_indices]\n",
    "text_y_train = text_y[train_indices]\n",
    "text_y_test = text_y[test_indices]\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "text_train_dataset = TextDataset(text_X_train, text_y_train)\n",
    "text_test_dataset = TextDataset(text_X_test, text_y_test)\n",
    "\n",
    "text_train_loader = DataLoader(text_train_dataset, batch_size=64, shuffle=False)\n",
    "text_test_loader = DataLoader(text_test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Combined dataset\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, image_dataset, text_dataset, p_image=0.33, p_text=0.33, p_both=0.34):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dataset: Dataset containing images.\n",
    "            text_dataset: Dataset containing texts.\n",
    "            p_image: Probability of returning only the image.\n",
    "            p_text: Probability of returning only the text.\n",
    "            p_both: Probability of returning both image and text.\n",
    "        \"\"\"\n",
    "        assert abs(p_image + p_text + p_both - 1.0) < 1e-6, \"Probabilities must sum to 1\"\n",
    "        self.image_dataset = image_dataset\n",
    "        self.text_dataset = text_dataset\n",
    "        self.p_image = p_image\n",
    "        self.p_text = p_text\n",
    "        self.p_both = p_both\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.image_dataset), len(self.text_dataset))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, image_label = self.image_dataset[idx]\n",
    "        text, text_label = self.text_dataset[idx]\n",
    "        assert image_label == text_label, \"Labels for image and text must match\"\n",
    "\n",
    "        mode = random.choices(['image', 'text', 'both'], weights=[self.p_image, self.p_text, self.p_both])[0]\n",
    "\n",
    "        if mode == 'image':\n",
    "            # Dummy text tensor\n",
    "            text = torch.zeros(MAX_LENGTH).long()\n",
    "        elif mode == 'text':\n",
    "            # Dummy image tensor\n",
    "            image = torch.zeros(3, 224, 224)\n",
    "        # Return selected data\n",
    "        return image, text, image_label\n",
    "\n",
    "combined_train_dataset = CombinedDataset(image_train_dataset, text_train_dataset)\n",
    "combined_test_dataset = CombinedDataset(image_test_dataset, text_test_dataset)\n",
    "combined_test_dataset_both = CombinedDataset(image_test_dataset, text_test_dataset, p_image=0, p_text=0, p_both=1)\n",
    "combined_test_dataset_image = CombinedDataset(image_test_dataset, text_test_dataset, p_image=1, p_text=0, p_both=0)\n",
    "combined_test_dataset_text = CombinedDataset(image_test_dataset, text_test_dataset, p_image=0, p_text=1, p_both=0)\n",
    "\n",
    "combined_train_loader = DataLoader(combined_train_dataset, batch_size=64, shuffle=True)\n",
    "combined_test_loader = DataLoader(combined_test_dataset, batch_size=64, shuffle=True)\n",
    "combined_test_loader_both = DataLoader(combined_test_dataset_both, batch_size=64, shuffle=True)\n",
    "combined_test_loader_image = DataLoader(combined_test_dataset_image, batch_size=64, shuffle=True)\n",
    "combined_test_loader_text = DataLoader(combined_test_dataset_text, batch_size=64, shuffle=True)\n",
    "\n",
    "# Parameters\n",
    "text_vocab_size = VOCAB_SIZE\n",
    "text_embedding_dim = 128\n",
    "text_seq_length = MAX_LENGTH\n",
    "num_classes = len(image_categories)\n",
    "\n",
    "print(\"Data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfb3dc3d-6f89-4a01-b828-3932766eee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch [1/10], Loss: 1.0691\n",
      "Evaluation (mixed):\n",
      "Accuracy: 0.6959\n",
      "Precision: 0.7036758072591334\n",
      "Recall: 0.6959\n",
      "F1 score: 0.6961703189026446\n",
      "\n",
      "\n",
      "Classification report:\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      amazon home       0.68      0.58      0.63      2017\n",
      "       automotive       0.69      0.85      0.77      2020\n",
      "             baby       0.79      0.71      0.75      1991\n",
      "     pet supplies       0.82      0.74      0.78      1983\n",
      "sports & outdoors       0.53      0.59      0.56      1989\n",
      "\n",
      "         accuracy                           0.70     10000\n",
      "        macro avg       0.70      0.70      0.70     10000\n",
      "     weighted avg       0.70      0.70      0.70     10000\n",
      "\n",
      "\n",
      "\n",
      "Epoch [2/10], Loss: 0.7496\n",
      "Evaluation (mixed):\n",
      "Accuracy: 0.7506\n",
      "Precision: 0.7523422727360207\n",
      "Recall: 0.7506\n",
      "F1 score: 0.747924734622753\n",
      "\n",
      "\n",
      "Classification report:\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      amazon home       0.76      0.68      0.72      2017\n",
      "       automotive       0.73      0.86      0.79      2020\n",
      "             baby       0.87      0.77      0.82      1991\n",
      "     pet supplies       0.74      0.86      0.80      1983\n",
      "sports & outdoors       0.66      0.58      0.62      1989\n",
      "\n",
      "         accuracy                           0.75     10000\n",
      "        macro avg       0.75      0.75      0.75     10000\n",
      "     weighted avg       0.75      0.75      0.75     10000\n",
      "\n",
      "\n",
      "\n",
      "Epoch [3/10], Loss: 0.6443\n",
      "Evaluation (mixed):\n",
      "Accuracy: 0.7785\n",
      "Precision: 0.7769239007012508\n",
      "Recall: 0.7785\n",
      "F1 score: 0.7755490619739448\n",
      "\n",
      "\n",
      "Classification report:\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      amazon home       0.78      0.72      0.75      2017\n",
      "       automotive       0.77      0.84      0.81      2020\n",
      "             baby       0.79      0.86      0.82      1991\n",
      "     pet supplies       0.80      0.85      0.82      1983\n",
      "sports & outdoors       0.74      0.62      0.67      1989\n",
      "\n",
      "         accuracy                           0.78     10000\n",
      "        macro avg       0.78      0.78      0.78     10000\n",
      "     weighted avg       0.78      0.78      0.78     10000\n",
      "\n",
      "\n",
      "\n",
      "Epoch [4/10], Loss: 0.5844\n",
      "Evaluation (mixed):\n",
      "Accuracy: 0.792\n",
      "Precision: 0.7974099502751119\n",
      "Recall: 0.792\n",
      "F1 score: 0.793481507788357\n",
      "\n",
      "\n",
      "Classification report:\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      amazon home       0.70      0.79      0.74      2017\n",
      "       automotive       0.80      0.83      0.82      2020\n",
      "             baby       0.88      0.82      0.85      1991\n",
      "     pet supplies       0.89      0.81      0.85      1983\n",
      "sports & outdoors       0.71      0.72      0.72      1989\n",
      "\n",
      "         accuracy                           0.79     10000\n",
      "        macro avg       0.80      0.79      0.79     10000\n",
      "     weighted avg       0.80      0.79      0.79     10000\n",
      "\n",
      "\n",
      "\n",
      "Epoch [5/10], Loss: 0.5424\n",
      "Evaluation (mixed):\n",
      "Accuracy: 0.7848\n",
      "Precision: 0.7874122432130889\n",
      "Recall: 0.7848\n",
      "F1 score: 0.783266661265578\n",
      "\n",
      "\n",
      "Classification report:\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      amazon home       0.74      0.76      0.75      2017\n",
      "       automotive       0.74      0.88      0.81      2020\n",
      "             baby       0.87      0.80      0.83      1991\n",
      "     pet supplies       0.82      0.85      0.83      1983\n",
      "sports & outdoors       0.77      0.64      0.70      1989\n",
      "\n",
      "         accuracy                           0.78     10000\n",
      "        macro avg       0.79      0.78      0.78     10000\n",
      "     weighted avg       0.79      0.78      0.78     10000\n",
      "\n",
      "\n",
      "\n",
      "Epoch [6/10], Loss: 0.5093\n",
      "Evaluation (mixed):\n",
      "Accuracy: 0.8054\n",
      "Precision: 0.8118873707584582\n",
      "Recall: 0.8054\n",
      "F1 score: 0.8069107059552701\n",
      "\n",
      "\n",
      "Classification report:\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      amazon home       0.76      0.77      0.76      2017\n",
      "       automotive       0.89      0.78      0.83      2020\n",
      "             baby       0.88      0.83      0.85      1991\n",
      "     pet supplies       0.84      0.86      0.85      1983\n",
      "sports & outdoors       0.69      0.80      0.74      1989\n",
      "\n",
      "         accuracy                           0.81     10000\n",
      "        macro avg       0.81      0.81      0.81     10000\n",
      "     weighted avg       0.81      0.81      0.81     10000\n",
      "\n",
      "\n",
      "\n",
      "Epoch [7/10], Loss: 0.4977\n",
      "Evaluation (mixed):\n",
      "Accuracy: 0.8045\n",
      "Precision: 0.8036758766644115\n",
      "Recall: 0.8045\n",
      "F1 score: 0.803994838069918\n",
      "\n",
      "\n",
      "Classification report:\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      amazon home       0.77      0.76      0.77      2017\n",
      "       automotive       0.82      0.83      0.83      2020\n",
      "             baby       0.85      0.86      0.86      1991\n",
      "     pet supplies       0.83      0.85      0.84      1983\n",
      "sports & outdoors       0.74      0.72      0.73      1989\n",
      "\n",
      "         accuracy                           0.80     10000\n",
      "        macro avg       0.80      0.80      0.80     10000\n",
      "     weighted avg       0.80      0.80      0.80     10000\n",
      "\n",
      "\n",
      "\n",
      "Epoch [8/10], Loss: 0.4695\n",
      "Evaluation (mixed):\n",
      "Accuracy: 0.8089\n",
      "Precision: 0.8120997198832288\n",
      "Recall: 0.8089\n",
      "F1 score: 0.8098077251147483\n",
      "\n",
      "\n",
      "Classification report:\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      amazon home       0.79      0.75      0.77      2017\n",
      "       automotive       0.83      0.83      0.83      2020\n",
      "             baby       0.87      0.85      0.86      1991\n",
      "     pet supplies       0.87      0.83      0.85      1983\n",
      "sports & outdoors       0.70      0.78      0.73      1989\n",
      "\n",
      "         accuracy                           0.81     10000\n",
      "        macro avg       0.81      0.81      0.81     10000\n",
      "     weighted avg       0.81      0.81      0.81     10000\n",
      "\n",
      "\n",
      "\n",
      "Epoch [9/10], Loss: 0.4495\n",
      "Evaluation (mixed):\n",
      "Accuracy: 0.8092\n",
      "Precision: 0.8154610438495841\n",
      "Recall: 0.8092\n",
      "F1 score: 0.810941666795759\n",
      "\n",
      "\n",
      "Classification report:\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      amazon home       0.75      0.78      0.77      2017\n",
      "       automotive       0.84      0.85      0.84      2020\n",
      "             baby       0.89      0.83      0.86      1991\n",
      "     pet supplies       0.90      0.80      0.85      1983\n",
      "sports & outdoors       0.70      0.79      0.74      1989\n",
      "\n",
      "         accuracy                           0.81     10000\n",
      "        macro avg       0.82      0.81      0.81     10000\n",
      "     weighted avg       0.82      0.81      0.81     10000\n",
      "\n",
      "\n",
      "\n",
      "Epoch [10/10], Loss: 0.4433\n",
      "Evaluation (mixed):\n",
      "Accuracy: 0.812\n",
      "Precision: 0.8139317042365195\n",
      "Recall: 0.812\n",
      "F1 score: 0.8114633403471565\n",
      "\n",
      "\n",
      "Classification report:\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      amazon home       0.81      0.75      0.78      2017\n",
      "       automotive       0.78      0.89      0.83      2020\n",
      "             baby       0.84      0.87      0.86      1991\n",
      "     pet supplies       0.89      0.81      0.85      1983\n",
      "sports & outdoors       0.75      0.73      0.74      1989\n",
      "\n",
      "         accuracy                           0.81     10000\n",
      "        macro avg       0.81      0.81      0.81     10000\n",
      "     weighted avg       0.81      0.81      0.81     10000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating on test data...\n",
      "Evaluation (image and text):\n",
      "Accuracy: 0.9015\n",
      "Precision: 0.9024177470114862\n",
      "Recall: 0.9015\n",
      "F1 score: 0.9018156397491135\n",
      "\n",
      "\n",
      "Classification report:\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      amazon home       0.87      0.87      0.87      2017\n",
      "       automotive       0.92      0.92      0.92      2020\n",
      "             baby       0.92      0.94      0.93      1991\n",
      "     pet supplies       0.97      0.93      0.95      1983\n",
      "sports & outdoors       0.83      0.85      0.84      1989\n",
      "\n",
      "         accuracy                           0.90     10000\n",
      "        macro avg       0.90      0.90      0.90     10000\n",
      "     weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "\n",
      "\n",
      "Evaluation (mixed):\n",
      "Accuracy: 0.8119\n",
      "Precision: 0.8136456021865193\n",
      "Recall: 0.8119\n",
      "F1 score: 0.8112752940666452\n",
      "\n",
      "\n",
      "Classification report:\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      amazon home       0.81      0.75      0.78      2017\n",
      "       automotive       0.78      0.89      0.83      2020\n",
      "             baby       0.84      0.88      0.86      1991\n",
      "     pet supplies       0.89      0.81      0.85      1983\n",
      "sports & outdoors       0.75      0.73      0.74      1989\n",
      "\n",
      "         accuracy                           0.81     10000\n",
      "        macro avg       0.81      0.81      0.81     10000\n",
      "     weighted avg       0.81      0.81      0.81     10000\n",
      "\n",
      "\n",
      "\n",
      "Evaluation (image only):\n",
      "Accuracy: 0.6415\n",
      "Precision: 0.6497330634113446\n",
      "Recall: 0.6415\n",
      "F1 score: 0.6364582256514513\n",
      "\n",
      "\n",
      "Classification report:\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      amazon home       0.71      0.53      0.61      2017\n",
      "       automotive       0.57      0.82      0.67      2020\n",
      "             baby       0.70      0.78      0.74      1991\n",
      "     pet supplies       0.69      0.56      0.62      1983\n",
      "sports & outdoors       0.58      0.51      0.54      1989\n",
      "\n",
      "         accuracy                           0.64     10000\n",
      "        macro avg       0.65      0.64      0.64     10000\n",
      "     weighted avg       0.65      0.64      0.64     10000\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/23 04:53:05 WARNING mlflow.utils.requirements_utils: Found torch version (2.7.0.dev20250122+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.7.0.dev20250122' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation (text only):\n",
      "Accuracy: 0.8879\n",
      "Precision: 0.8893982029171351\n",
      "Recall: 0.8879\n",
      "F1 score: 0.8884644965747488\n",
      "\n",
      "\n",
      "Classification report:\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      amazon home       0.84      0.85      0.85      2017\n",
      "       automotive       0.91      0.91      0.91      2020\n",
      "             baby       0.92      0.92      0.92      1991\n",
      "     pet supplies       0.97      0.92      0.95      1983\n",
      "sports & outdoors       0.81      0.84      0.82      1989\n",
      "\n",
      "         accuracy                           0.89     10000\n",
      "        macro avg       0.89      0.89      0.89     10000\n",
      "     weighted avg       0.89      0.89      0.89     10000\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/23 04:53:09 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.22.0.dev20250122+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torchvision==0.22.0.dev20250122' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/01/23 04:53:09 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluation completed\n"
     ]
    }
   ],
   "source": [
    "# Model definition, training and eval\n",
    "# Model instantiation\n",
    "model = MultimodalClassifier(text_vocab_size, text_embedding_dim, text_seq_length, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Evaluation mixed (sometimes image+text, sometimes only image or text - like in training data)\n",
    "def evaluate_image_text(model, dataloader, device, classes, title):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for images, texts, labels in dataloader:\n",
    "            images, texts, labels = images.to(device), texts.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images, texts)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(title)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 score: {f1}\\n\\n\")\n",
    "    print(f\"Classification report:\\n\\n{classification_report(all_labels, all_preds, target_names=classes)}\\n\\n\")\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Evaluation image only\n",
    "def evaluate_image(model, dataloader, device, classes):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for images, texts, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Dummy tensor for text (since we're evaluating only with images)\n",
    "            dummy_text_tensor = torch.zeros(images.size(0), MAX_LENGTH).to(device).long()\n",
    "            \n",
    "            # Pass only the image and dummy text tensor to the model\n",
    "            outputs = model(images, dummy_text_tensor)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(\"Evaluation (image only):\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 score: {f1}\\n\\n\")\n",
    "    print(f\"Classification report:\\n\\n{classification_report(all_labels, all_preds, target_names=classes)}\\n\\n\")\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Evaluation text only\n",
    "def evaluate_text(model, dataloader, device, classes):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for images, texts, labels in dataloader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "            # Dummy tensor for image (since we're evaluating only with text)\n",
    "            dummy_image_tensor = torch.zeros(texts.size(0), 3, 224, 224).to(device)\n",
    "            \n",
    "            # Pass only the text and dummy image tensor to the model\n",
    "            outputs = model(dummy_image_tensor, texts)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(\"Evaluation (text only):\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 score: {f1}\\n\\n\")\n",
    "    print(f\"Classification report:\\n\\n{classification_report(all_labels, all_preds, target_names=classes)}\\n\\n\")\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Training loop\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, texts, labels in dataloader:\n",
    "        images, texts, labels = images.to(device), texts.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images, texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# mlflow auth\n",
    "# for production should be moved to env variables\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'iis_user'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = 'iis_password'\n",
    "\n",
    "mlflow.set_tracking_uri(\"https://bojan-radulovic.xyz/mlflow/\")\n",
    "with mlflow.start_run():\n",
    "    # Log hyperparameters\n",
    "    mlflow.log_param(\"text_vocab_size\", text_vocab_size)\n",
    "    mlflow.log_param(\"text_embedding_dim\", text_embedding_dim)\n",
    "    mlflow.log_param(\"text_seq_length\", text_seq_length)\n",
    "    mlflow.log_param(\"num_classes\", num_classes)\n",
    "\n",
    "    mlflow.log_param(\"loss_fun\", \"CrossEntropyLoss\")\n",
    "    mlflow.log_param(\"lr\", 0.001)\n",
    "\n",
    "    # Main execution loop\n",
    "    num_epochs = 10\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, combined_train_loader, criterion, optimizer, device)\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch+1)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # Evaluation on validation/test data\n",
    "        # There is no validation dataset so validation is also done on test dataset\n",
    "        val_accuracy, val_precision, val_recall, val_f1 = evaluate_image_text(\n",
    "            model, combined_test_loader, device, image_categories, \"Evaluation (mixed):\"\n",
    "        )\n",
    "        \n",
    "        # Log metrics for this epoch\n",
    "        mlflow.log_metric(\"val_accuracy\", val_accuracy, step=epoch)\n",
    "        mlflow.log_metric(\"val_precision\", val_precision, step=epoch)\n",
    "        mlflow.log_metric(\"val_recall\", val_recall, step=epoch)\n",
    "        mlflow.log_metric(\"val_f1\", val_f1, step=epoch)\n",
    "\n",
    "    # Log final metrics after evaluation\n",
    "    print(\"\\nEvaluating on test data...\")\n",
    "    accuracy_image_and_text, precision_image_and_text, recall_image_and_text, f1_image_and_text = evaluate_image_text(model, combined_test_loader_both, device, image_categories, \"Evaluation (image and text):\")\n",
    "    accuracy_mixed, precision_mixed, recall_mixed, f1_mixed = evaluate_image_text(model, combined_test_loader, device, image_categories, \"Evaluation (mixed):\")\n",
    "    accuracy_image, precision_image, recall_image, f1_image = evaluate_image(model, combined_test_loader_image, device, image_categories)\n",
    "    accuracy_text, precision_text, recall_text, f1_text = evaluate_text(model, combined_test_loader_text, device, image_categories)\n",
    "\n",
    "    mlflow.log_metric(\"test_accuracy_image_and_text\", accuracy_image_and_text)\n",
    "    mlflow.log_metric(\"test_precision_image_and_text\", precision_image_and_text)\n",
    "    mlflow.log_metric(\"test_recall_image_and_text\", recall_image_and_text)\n",
    "    mlflow.log_metric(\"test_f1_image_and_text\", f1_image_and_text)\n",
    "    \n",
    "    mlflow.log_metric(\"test_accuracy_mixed\", accuracy_mixed)\n",
    "    mlflow.log_metric(\"test_precision_mixed\", precision_mixed)\n",
    "    mlflow.log_metric(\"test_recall_mixed\", recall_mixed)\n",
    "    mlflow.log_metric(\"test_f1_mixed\", f1_mixed)\n",
    "\n",
    "    mlflow.log_metric(\"test_accuracy_image\", accuracy_image)\n",
    "    mlflow.log_metric(\"test_precision_image\", precision_image)\n",
    "    mlflow.log_metric(\"test_recall_image\", recall_image)\n",
    "    mlflow.log_metric(\"test_f1_image\", f1_image)\n",
    "\n",
    "    mlflow.log_metric(\"test_accuracy_text\", accuracy_text)\n",
    "    mlflow.log_metric(\"test_precision_text\", precision_text)\n",
    "    mlflow.log_metric(\"test_recall_text\", recall_text)\n",
    "    mlflow.log_metric(\"test_f1_text\", f1_text)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n",
    "print(\"Training and evaluation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4efcf952-f4d2-4c6f-ad6e-0359a243ba24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./multimodal_classifier_10.pth\n"
     ]
    }
   ],
   "source": [
    "#model saving\n",
    "save_path = './multimodal_classifier_10.pth'\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1eab6f8b-735c-4b55-b402-1782b2ccf5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class (image + text): pet supplies\n",
      "Predicted class (image only): pet supplies\n",
      "Predicted class (text only): pet supplies\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "# Preprocessing function for the image\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Inference function\n",
    "def predict(image_path=None, text=None, model=None, device=None, categories=None):\n",
    "    image_tensor = None\n",
    "    text_tensor = None\n",
    "    \n",
    "    # If the image is provided, load and transform it\n",
    "    if image_path:\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Image not found: {image_path}\")\n",
    "            return None\n",
    "        image_tensor = image_transform(image).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "    \n",
    "    # If the text is provided, tokenize it\n",
    "    if text:\n",
    "        text_tokens = tokenize_text(text, MAX_LENGTH)\n",
    "        text_tensor = torch.tensor(text_tokens).unsqueeze(0).to(device).long()  # Ensure it's Long for text input\n",
    "    \n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        if image_tensor is not None and text_tensor is not None:\n",
    "            output = model(image_tensor, text_tensor)\n",
    "        elif image_tensor is not None:  # Only image input\n",
    "            output = model(image_tensor, torch.zeros(1, MAX_LENGTH).to(device).long())  # Dummy tensor for text (Long type)\n",
    "        elif text_tensor is not None:  # Only text input\n",
    "            output = model(torch.zeros(1, 3, 224, 224).to(device).float(), text_tensor)  # Dummy tensor for image (Float type)\n",
    "        else:\n",
    "            print(\"Both image and text are missing.\")\n",
    "            return None\n",
    "\n",
    "        _, predicted_class = torch.max(output, 1)\n",
    "    \n",
    "    # Get the class name from the index\n",
    "    predicted_class_name = categories[predicted_class.item()]\n",
    "    \n",
    "    return predicted_class_name\n",
    "\n",
    "# Example usage:\n",
    "image_path = \"datasets/dataset_clean/images/product_299.jpg\"\n",
    "text_input = \"Pet dog collar for your little dog.\"\n",
    "\n",
    "# Predicting with both image and text\n",
    "predicted_class_name = predict(image_path=image_path, text=text_input, model=model, device=device, categories=image_categories)\n",
    "print(f\"Predicted class (image + text): {predicted_class_name}\")\n",
    "\n",
    "# Predicting with only the image\n",
    "predicted_class_name = predict(image_path=image_path, text=None, model=model, device=device, categories=image_categories)\n",
    "print(f\"Predicted class (image only): {predicted_class_name}\")\n",
    "\n",
    "# Predicting with only the text\n",
    "predicted_class_name = predict(image_path=None, text=text_input, model=model, device=device, categories=image_categories)\n",
    "print(f\"Predicted class (text only): {predicted_class_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
